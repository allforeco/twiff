{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af97d86",
   "metadata": {},
   "source": [
    "# Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e62e462d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 03/23/2022 02:35:11PM ] __main__ | INFO | Executing: ['C:\\\\Users\\\\samca\\\\anaconda3\\\\envs\\\\twiff\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\samca\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-5cc6faf1-6a29-4775-8626-716ac6f51993.json']\n",
      "[ 03/23/2022 02:35:11PM ] __main__ | INFO | Loaded execution configuration {'search': {'module': 'twiff.search', 'call': 'search', 'config': {'cursor': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\cursor.json', 'query': '(#twiff OR #Twiff) -is:retweet -from:twiff_bot', 'max_requests': 100}}, 'like': {'module': 'twiff.search', 'call': 'like', 'config': {'max_requests': 100}}, 'parse': {'module': 'twiff.parse', 'call': 'parse_tweets'}, 'respond': {'module': 'twiff.reply', 'call': 'ReplyGenerator', 'config': {'path': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\responses.json'}}, 'export': {'module': 'twiff.io', 'call': 'dump_json_items', 'config': {'output': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\dump'}}} from 0\n",
      "[ 03/23/2022 02:35:11PM ] twiff.search | INFO | Running search using following arguments: Namespace(config=None, keys=None, search={'module': 'twiff.search', 'call': 'search', 'config': {'cursor': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\cursor.json', 'query': '(#twiff OR #Twiff) -is:retweet -from:twiff_bot', 'max_requests': 100}}, like={'module': 'twiff.search', 'call': 'like', 'config': {'max_requests': 100}}, parse={'module': 'twiff.parse', 'call': 'parse_tweets'}, respond={'module': 'twiff.reply', 'call': 'ReplyGenerator', 'config': {'path': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\responses.json'}}, export={'module': 'twiff.io', 'call': 'dump_json_items', 'config': {'output': 'C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\dump'}})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20192/748646801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0marg_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Python Projects\\twiff\\src\\twiff\\search.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;31m# Log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Running search using following arguments: {args}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# Configuration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(format=\"[ %(asctime)s ] %(name)s | %(levelname)s | %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S%p\", level=logging.INFO)\n",
    "log.info(f\"Executing: {sys.argv}\")\n",
    "\n",
    "# Ensure src path is in sys path\n",
    "base_path = os.path.dirname(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\src\\\\twiff\")\n",
    "if not base_path in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "\n",
    "# Load configuration for run\n",
    "with open(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\search.json\", 'r') as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "# Run requested mode\n",
    "if True:\n",
    "    from twiff import search\n",
    "    arg_parser = search.get_arg_parser()\n",
    "    arg_parser.set_defaults(**config)\n",
    "    args = arg_parser.parse_args([])\n",
    "    tweets, users, errors = search.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3fe2f67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1504837019427819522': ['created_at: 2022-03-18T15:07:58.000Z', \"entities: {'urls': [{'start': 276, 'end': 299, 'url': 'https://t.co/aSfrB44UeQ', 'expanded_url': 'https://twitter.com/Nasadox_/status/1504837019427819522/photo/1', 'display_url': 'pic.twitter.com/aSfrB44UeQ'}], 'mentions': [{'start': 222, 'end': 235, 'username': 'ECOWARRIORSS', 'id': '2184934963'}, {'start': 237, 'end': 250, 'username': 'biofuelwatch', 'id': '174979306'}, {'start': 252, 'end': 265, 'username': 'sustainme_in', 'id': '1166444989791948800'}, {'start': 267, 'end': 275, 'username': 'Sdg13Un', 'id': '1404797639040843784'}], 'hashtags': [{'start': 0, 'end': 14, 'tag': 'ClimateStrike'}, {'start': 24, 'end': 31, 'tag': 'reduce'}, {'start': 32, 'end': 38, 'tag': 'reuse'}, {'start': 39, 'end': 49, 'tag': 'recycling'}, {'start': 97, 'end': 116, 'tag': 'GlobalRecyclingDay'}, {'start': 180, 'end': 197, 'tag': 'FridaysForFuture'}, {'start': 198, 'end': 204, 'tag': 'twiff'}, {'start': 206, 'end': 221, 'tag': 'climatehopeful'}]}\", 'reply_settings: everyone', 'text: #ClimateStrike Week 30 \\n#reduce #reuse #recycling is not (also an Option) it is\\nThe Need now‚ùï\\nOn #GlobalRecyclingDay ‚ôªwe all must ensure \\nOur Earthüåéshould not be Constricted more‚ùó\\n#FridaysForFuture\\n#twiff \\n#climatehopeful\\n@ECOWARRIORSS \\n@biofuelwatch \\n@sustainme_in \\n@Sdg13Un https://t.co/aSfrB44UeQ', 'source: Twitter Web App', 'id: 1504837019427819522', 'lang: en', 'author_id: 1048180379847258112', 'conversation_id: 1504837019427819522'],\n",
       " '1504760416609648640': ['created_at: 2022-03-18T10:03:35.000Z', \"entities: {'urls': [{'start': 207, 'end': 230, 'url': 'https://t.co/j2uyOupyix', 'expanded_url': 'https://twitter.com/auber_fichess/status/1504728815746048008/photo/1', 'display_url': 'pic.twitter.com/j2uyOupyix'}], 'mentions': [{'start': 18, 'end': 32, 'username': 'auber_fichess', 'id': '939043659068166144'}], 'hashtags': [{'start': 42, 'end': 56, 'tag': 'ClimateStrike'}, {'start': 60, 'end': 67, 'tag': 'Angola'}, {'start': 68, 'end': 85, 'tag': 'FridaysForFuture'}, {'start': 86, 'end': 102, 'tag': 'UprootTheSystem'}, {'start': 103, 'end': 127, 'tag': 'FaceTheClimateEmergency'}, {'start': 128, 'end': 144, 'tag': 'PeopleNotProfit'}, {'start': 145, 'end': 151, 'tag': 'twiff'}]}\", 'reply_settings: everyone', 'text: GretaThunberg: RT @auber_fichess: Week 40 #ClimateStrike in #Angola\\n#FridaysForFuture\\n#UprootTheSystem\\n#FaceTheClimateEmergency\\n#PeopleNotProfit\\n#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/j2uyOupyix', 'source: IFTTT', 'id: 1504760416609648640', 'lang: it', 'author_id: 115902452', 'conversation_id: 1504760416609648640'],\n",
       " '1504751803757481990': ['created_at: 2022-03-18T09:29:21.000Z', \"entities: {'urls': [{'start': 202, 'end': 225, 'url': 'https://t.co/Hzbvzavuxc', 'expanded_url': 'https://twitter.com/fischerdata/status/1504751803757481990/photo/1', 'display_url': 'pic.twitter.com/Hzbvzavuxc'}], 'mentions': [{'start': 103, 'end': 113, 'username': 'Bundestag', 'id': '3088296873'}, {'start': 132, 'end': 144, 'username': 'FFFMapCount', 'id': '1420433709992448001'}], 'hashtags': [{'start': 64, 'end': 79, 'tag': 'ParisAgreement'}, {'start': 85, 'end': 102, 'tag': 'FridaysForFuture'}, {'start': 125, 'end': 131, 'tag': 'twiff'}, {'start': 184, 'end': 201, 'tag': 'ClimateActionNow'}]}\", 'reply_settings: everyone', 'text: Es ist heute etwas umst√§ndlicher, aber wir sind wieder hier f√ºr #ParisAgreement  f√ºr #FridaysForFuture @Bundestag  10 bis 12 #twiff @FFFMapCount 2022.03.18, 5 personen, Berlin Germany #ClimateActionNow https://t.co/Hzbvzavuxc', 'source: Twitter for Android', 'id: 1504751803757481990', 'lang: de', 'author_id: 3351746919', 'conversation_id: 1504751803757481990'],\n",
       " '1504748042628521988': ['created_at: 2022-03-18T09:14:24.000Z', \"entities: {'urls': [{'start': 279, 'end': 302, 'url': 'https://t.co/YHWN53EbI4', 'expanded_url': 'https://twitter.com/JanineClimate/status/1504748042628521988/photo/1', 'display_url': 'pic.twitter.com/YHWN53EbI4'}], 'hashtags': [{'start': 73, 'end': 83, 'tag': 'PutinsWar'}, {'start': 90, 'end': 104, 'tag': 'ClimateCrisis'}, {'start': 144, 'end': 156, 'tag': 'FossilFuels'}, {'start': 178, 'end': 192, 'tag': 'ClimateCrisis'}, {'start': 193, 'end': 203, 'tag': 'StopEACOP'}, {'start': 204, 'end': 212, 'tag': 'ecocide'}, {'start': 213, 'end': 229, 'tag': 'PeopleNotProfit'}, {'start': 231, 'end': 237, 'tag': 'twiff'}, {'start': 242, 'end': 259, 'tag': 'FridaysForFuture'}, {'start': 260, 'end': 267, 'tag': 'Sweden'}]}\", 'reply_settings: everyone', 'text: Why do governments say climate can wait until 2030 when people die now?\\n\\n#PutinsWar &amp; #ClimateCrisis are moral failures, illegal &amp; need #FossilFuels!\\n\\nWeek 187 March 18\\n\\n#ClimateCrisis #StopEACOP #ecocide #PeopleNotProfit\\n\\n#twiff, 2, #FridaysForFuture #Sweden,, Svedmyra https://t.co/YHWN53EbI4', 'source: Twitter for Android', 'id: 1504748042628521988', 'lang: en', 'author_id: 113721008', 'conversation_id: 1504748042628521988'],\n",
       " '1504728815746048008': ['created_at: 2022-03-18T07:58:00.000Z', \"entities: {'urls': [{'start': 173, 'end': 196, 'url': 'https://t.co/mh5TAvTe75', 'expanded_url': 'https://twitter.com/auber_fichess/status/1504728815746048008/photo/1', 'display_url': 'pic.twitter.com/mh5TAvTe75'}], 'hashtags': [{'start': 8, 'end': 22, 'tag': 'ClimateStrike'}, {'start': 26, 'end': 33, 'tag': 'Angola'}, {'start': 34, 'end': 51, 'tag': 'FridaysForFuture'}, {'start': 52, 'end': 68, 'tag': 'UprootTheSystem'}, {'start': 69, 'end': 93, 'tag': 'FaceTheClimateEmergency'}, {'start': 94, 'end': 110, 'tag': 'PeopleNotProfit'}, {'start': 111, 'end': 117, 'tag': 'twiff'}]}\", 'reply_settings: everyone', 'text: Week 40 #ClimateStrike in #Angola\\n#FridaysForFuture\\n#UprootTheSystem\\n#FaceTheClimateEmergency\\n#PeopleNotProfit\\n#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/mh5TAvTe75', 'source: Twitter for Android', 'id: 1504728815746048008', 'lang: it', 'author_id: 939043659068166144', 'conversation_id: 1504728815746048008']}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "658a21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ClimateStrike Week 30 \n",
      "#reduce #reuse #recycling is not (also an Option) it is\n",
      "The Need now‚ùï\n",
      "On #GlobalRecyclingDay ‚ôªwe all must ensure \n",
      "Our Earthüåéshould not be Constricted more‚ùó\n",
      "#FridaysForFuture\n",
      "#twiff \n",
      "#climatehopeful\n",
      "@ECOWARRIORSS \n",
      "@biofuelwatch \n",
      "@sustainme_in \n",
      "@Sdg13Un https://t.co/aSfrB44UeQ\n",
      "None\n",
      "-----\n",
      "GretaThunberg: RT @auber_fichess: Week 40 #ClimateStrike in #Angola\n",
      "#FridaysForFuture\n",
      "#UprootTheSystem\n",
      "#FaceTheClimateEmergency\n",
      "#PeopleNotProfit\n",
      "#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/j2uyOupyix\n",
      "{'num_people': '1', 'created_at': '2022-03-18T10:03:35.000Z', 'organization': 'fridays 4 future angola', 'location': 'luanda, angola'}\n",
      "-----\n",
      "Es ist heute etwas umst√§ndlicher, aber wir sind wieder hier f√ºr #ParisAgreement  f√ºr #FridaysForFuture @Bundestag  10 bis 12 #twiff @FFFMapCount 2022.03.18, 5 personen, Berlin Germany #ClimateActionNow https://t.co/Hzbvzavuxc\n",
      "None\n",
      "-----\n",
      "Why do governments say climate can wait until 2030 when people die now?\n",
      "\n",
      "#PutinsWar &amp; #ClimateCrisis are moral failures, illegal &amp; need #FossilFuels!\n",
      "\n",
      "Week 187 March 18\n",
      "\n",
      "#ClimateCrisis #StopEACOP #ecocide #PeopleNotProfit\n",
      "\n",
      "#twiff, 2, #FridaysForFuture #Sweden,, Svedmyra https://t.co/YHWN53EbI4\n",
      "{'num_people': '2', 'created_at': '2022-03-18T09:14:24.000Z', 'organization': '#fridaysforfuture #sweden', 'location': 'svedmyra'}\n",
      "-----\n",
      "Week 40 #ClimateStrike in #Angola\n",
      "#FridaysForFuture\n",
      "#UprootTheSystem\n",
      "#FaceTheClimateEmergency\n",
      "#PeopleNotProfit\n",
      "#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/mh5TAvTe75\n",
      "{'num_people': '1', 'created_at': '2022-03-18T07:58:00.000Z', 'organization': 'fridays 4 future angola', 'location': 'luanda, angola'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from twiff.parse import parse_tweets\n",
    "\n",
    "for (tweet, parsed) in zip(tweets.values(), parse_tweets(tweets).values()):\n",
    "    print(tweet.text)\n",
    "    print(parsed)\n",
    "    \n",
    "    print('-----')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe48af",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc10aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8f21d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1504728815746048008 @ 2022-03-18T07:58:00.000Z by 0\u001b[0m\n",
      "Week 40 #ClimateStrike in #Angola\n",
      "#FridaysForFuture\n",
      "#UprootTheSystem\n",
      "#FaceTheClimateEmergency\n",
      "#PeopleNotProfit\n",
      "#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/mh5TAvTe75\n",
      "----------\n",
      "\n",
      "\u001b[1m1504748042628521988 @ 2022-03-18T09:14:24.000Z by 0\u001b[0m\n",
      "Why do governments say climate can wait until 2030 when people die now?\n",
      "\n",
      "#PutinsWar &amp; #ClimateCrisis are moral failures, illegal &amp; need #FossilFuels!\n",
      "\n",
      "Week 187 March 18\n",
      "\n",
      "#ClimateCrisis #StopEACOP #ecocide #PeopleNotProfit\n",
      "\n",
      "#twiff, 2, #FridaysForFuture #Sweden,, Svedmyra https://t.co/YHWN53EbI4\n",
      "----------\n",
      "\n",
      "\u001b[1m1504751803757481990 @ 2022-03-18T09:29:21.000Z by 0\u001b[0m\n",
      "Es ist heute etwas umst√§ndlicher, aber wir sind wieder hier f√ºr #ParisAgreement  f√ºr #FridaysForFuture @Bundestag  10 bis 12 #twiff @FFFMapCount 2022.03.18, 5 personen, Berlin Germany #ClimateActionNow https://t.co/Hzbvzavuxc\n",
      "----------\n",
      "\n",
      "\u001b[1m1504760416609648640 @ 2022-03-18T10:03:35.000Z by 0\u001b[0m\n",
      "GretaThunberg: RT @auber_fichess: Week 40 #ClimateStrike in #Angola\n",
      "#FridaysForFuture\n",
      "#UprootTheSystem\n",
      "#FaceTheClimateEmergency\n",
      "#PeopleNotProfit\n",
      "#twiff 1, Fridays 4 Future Angola, Luanda,Angola, 18, 03,2022 https://t.co/j2uyOupyix\n",
      "----------\n",
      "\n",
      "\u001b[1m1504837019427819522 @ 2022-03-18T15:07:58.000Z by 0\u001b[0m\n",
      "#ClimateStrike Week 30 \n",
      "#reduce #reuse #recycling is not (also an Option) it is\n",
      "The Need now‚ùï\n",
      "On #GlobalRecyclingDay ‚ôªwe all must ensure \n",
      "Our Earthüåéshould not be Constricted more‚ùó\n",
      "#FridaysForFuture\n",
      "#twiff \n",
      "#climatehopeful\n",
      "@ECOWARRIORSS \n",
      "@biofuelwatch \n",
      "@sustainme_in \n",
      "@Sdg13Un https://t.co/aSfrB44UeQ\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in list(tweets.values())[::-1]: \n",
    "    print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, 0) + color.END)\n",
    "    print(tweet.text)\n",
    "    print('----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e5d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "      u\"\\U0001F600-\\U0001F64F\"\n",
    "      u\"\\U0001F300-\\U0001F5FF\"\n",
    "      u\"\\U0001F680-\\U0001F6FF\"\n",
    "      u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "      u\"\\U00002500-\\U00002BEF\"\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U000024C2-\\U0001F251\"\n",
    "      u\"\\U0001f926-\\U0001f937\"\n",
    "      u\"\\U00010000-\\U0010ffff\"\n",
    "      u\"\\u2640-\\u2642\"\n",
    "      u\"\\u2600-\\u2B55\"\n",
    "      u\"\\u200d\"\n",
    "      u\"\\u23cf\"\n",
    "      u\"\\u23e9\"\n",
    "      u\"\\u231a\"\n",
    "      u\"\\ufe0f\"\n",
    "      u\"\\u3030\"\n",
    "      \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f121d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\samca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c75ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from nltk.corpus import words\n",
    "\n",
    "setofwords = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb4b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "ls = ['a', 's', 'r', 'n', 'v']\n",
    "speech_constants = {}\n",
    "\n",
    "for constant in ls:\n",
    "    word_set = {word.name().split('.', 1)[0] for word in wn.all_synsets(constant)}\n",
    "    speech_constants = {*speech_constants, *word_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334cf33",
   "metadata": {},
   "source": [
    "## Strict Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab405c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import torchtext\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eb303a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_people_from_tokens(tokens):\n",
    "    # Search for first token with run of 1\n",
    "    run = 0\n",
    "    num_people = None\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        # Attempt to convert token to integer format - accumulate number of integers encountered.\n",
    "        try:\n",
    "            int(token)\n",
    "            run += 1\n",
    "        except:\n",
    "            if run==1:\n",
    "                num_people = tokens[tdx-1]\n",
    "                break\n",
    "            run = 0\n",
    "        \n",
    "    return num_people\n",
    "\n",
    "def remove_integer_tokens(tokens):   \n",
    "    # Remove any other integer from tokens\n",
    "    int_idxs = []\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        try:\n",
    "            int(token)\n",
    "            int_idxs.append(tdx)\n",
    "        except:\n",
    "            pass\n",
    "    for int_idx in int_idxs[::-1]:\n",
    "        del tokens[int_idx]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def remove_integers_from_tokens(tokens):\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        token = token.strip().lower()\n",
    "        token = remove_emoji(token)\n",
    "        token = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', token)\n",
    "        \n",
    "        sub_tokens = torchtext.data.get_tokenizer(\"basic_english\")(token)\n",
    "        \n",
    "        int_idxs = []\n",
    "        for idx, sub_token in enumerate(sub_tokens):\n",
    "            try:\n",
    "                int(sub_token)\n",
    "                int_idxs.append(idx)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for int_idx in int_idxs[::-1]:\n",
    "            del sub_tokens[int_idx]\n",
    "            \n",
    "        tokens[tdx] = ''.join(sub_tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c2d9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets(tweets, **kwargs):\n",
    "    parsed_tweets = {}\n",
    "\n",
    "    for idx, (id_str, tweet) in enumerate(tweets.items()):\n",
    "        # Retrieve text from tweet\n",
    "        text = tweet.text.lower()\n",
    "\n",
    "        # Remove data before '#twiff' token\n",
    "        token_idx = text.find('#twiff')\n",
    "        if token_idx:\n",
    "            text = text[token_idx+len('#twiff'):]\n",
    "\n",
    "        # Remove data after 'http' token\n",
    "        token_idx = text.find('http')\n",
    "        if token_idx:\n",
    "            text = text[:token_idx]\n",
    "\n",
    "        # Slight cleaning of tokens\n",
    "        tokens = [item.strip() for item in text.split(',') if item!='']\n",
    "\n",
    "        # Extract number of people\n",
    "        num_people = num_people_from_tokens(tokens)\n",
    "\n",
    "        # Remove integer tokens\n",
    "        tokens = remove_integer_tokens(tokens)\n",
    "\n",
    "        if num_people:\n",
    "            parsed_tweets[id_str] = {\"num_people\":num_people, \"created_at\":tweet.created_at, \"organization\":tokens[0], \"location\":', '.join(tokens[1:])}\n",
    "        else:\n",
    "            parsed_tweets[id_str] = None\n",
    "            \n",
    "    return parsed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3da444aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504837019427819522: None\n",
      "\n",
      "1504760416609648640: {'num_people': '1', 'created_at': '2022-03-18T10:03:35.000Z', 'organization': 'Fridays 4 Future Angola', 'location': 'Luanda, Angola'}\n",
      "\n",
      "1504751803757481990: None\n",
      "\n",
      "1504748042628521988: {'num_people': '2', 'created_at': '2022-03-18T09:14:24.000Z', 'organization': '#FridaysForFuture #Sweden', 'location': 'Svedmyra'}\n",
      "\n",
      "1504728815746048008: {'num_people': '1', 'created_at': '2022-03-18T07:58:00.000Z', 'organization': 'Fridays 4 Future Angola', 'location': 'Luanda, Angola'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_tweets = parse_tweets(tweets)\n",
    "\n",
    "for id_str, tweet in parsed_tweets.items():\n",
    "    print(f\"{id_str}: {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(client:Any, parsed_tweets:Any, response_generator:Callable) -> None:\n",
    "    \"\"\" Replies to the author of the parsed tweet as dictated by the provided response_generator.\n",
    "    \n",
    "        Args:\n",
    "            client (tweepy.Client): Registered and X client.\n",
    "            parsed_tweets (Dict): Parsed tweet data in dictionary format.\n",
    "                Key (str): Unique ID corresponding to the tweet.\n",
    "                Value (Dict):\n",
    "                    num_people (int):\n",
    "                    created_at (str):\n",
    "                    organization (str):\n",
    "                    location (str): \n",
    "            response_generator (Callable): Callable function to generate response based on parsed tweet data.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        Example::\n",
    "            >>> tweets = search(...)\n",
    "            >>> parsed_tweets = twiff.parse.parse_tweets(tweets)\n",
    "            >>> reply(client, parsed_tweets, ...) # ...\n",
    "    \"\"\"\n",
    "    for idx, (id_str, parsed_tweet) in enumerate(parsed_tweets.items()):\n",
    "        response = response_generator(parsed_tweet)\n",
    "        client.create_tweet(in_reply_to_tweet_id=tweet.id, text=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# tweet = list(results.tweets.values())[::-1][0]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text) # reply_settings=\"mentionedUsers\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1883c6",
   "metadata": {},
   "source": [
    "### old testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "efa0ccc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# valid_count = 0\n",
    "\n",
    "# strict_parse = False\n",
    "# attempt_clean_url_entities = True\n",
    "\n",
    "# organization_list = []\n",
    "# times = []\n",
    "\n",
    "# for idx, (key, tweet) in enumerate(list(tweets.items())[::-1]):\n",
    "#     start_time = time.time()\n",
    "#     could_parse = False \n",
    "    \n",
    "#     print(idx)\n",
    "#     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, 0) + color.END)\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     if hasattr(tweet, 'entities'):\n",
    "#         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "#     else:\n",
    "#         url_start_idx = -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     # Clean link from tokenized fields\n",
    "#     if 'https' in tokens:\n",
    "#         tokens = tokens[:tokens.index('https')]\n",
    "    \n",
    "#     twiff_idxs = [idx for (idx, token) in enumerate(tokens) if token=='twiff']\n",
    "    \n",
    "#     for idx in twiff_idxs:\n",
    "#         if idx+1<len(tokens):\n",
    "#             try:\n",
    "#                 int(tokens[idx+1])\n",
    "#                 tokens = tokens[idx+1:]\n",
    "#                 valid_count += 1\n",
    "                \n",
    "#                 # Parse number of people: Can verify\n",
    "#                 num_people = int(tokens[0])\n",
    "#                 del tokens[0]\n",
    "                \n",
    "#                 # remove other tokens\n",
    "#                 for token in tokens[::-1]:\n",
    "#                     try:\n",
    "#                         int(token)\n",
    "#                         del tokens[tokens.index(token)]\n",
    "#                     except ValueError as e:\n",
    "#                         None \n",
    "                \n",
    "#                 # Parse datetime\n",
    "#                 inferred_datetime = False\n",
    "#                 for idx, token in enumerate(tokens): \n",
    "#                     date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#                     if date is not None:  \n",
    "#                         inferred_datetime = True\n",
    "#                         del tokens[idx]\n",
    "#                         break\n",
    "#                 if date is not None:\n",
    "#                     date = color.GREEN + '{}'.format(date) + color.END\n",
    "#                 else:\n",
    "#                     date = color.YELLOW + '{}'.format(datetime.datetime.strptime(tweet.created_at, '%Y-%m-%dT%H:%M:%S.%fZ')) + color.END\n",
    "                    \n",
    "#                 # Extract organization\n",
    "#                 organization = tokens[0]\n",
    "#                 organization_list.append(organization)\n",
    "#                 del tokens[0]\n",
    "\n",
    "#                 # Extract location\n",
    "#                 if [idx for (idx, token) in enumerate(tokens) if token=='twiff']:\n",
    "#                     tokens = tokens[:[idx for (idx, token) in enumerate(tokens) if token=='twiff'][0]]\n",
    "#                 location = ', '.join(tokens)                 \n",
    "                \n",
    "#                 could_parse = True\n",
    "                \n",
    "#             except ValueError as e:\n",
    "#                 None \n",
    "                \n",
    "#     end_time = time.time()\n",
    "#     times.append(end_time-start_time)\n",
    "#     print('Parsing tweet took: {:.3f} seconds'.format(end_time-start_time))      \n",
    "    \n",
    "#     if could_parse:\n",
    "#         print(color.GREEN + 'Successfully parsed tweet.' + color.END)\n",
    "#         print(f'Number of People: {num_people}\\nOrganization: {organization}\\nLocation: {location}\\nDatetime: {date}\\n')\n",
    "#     else:\n",
    "#         print(color.RED + 'Unable to parse: {}...'.format(tweet.text[:100]) + color.END)\n",
    "#         print('Tokenized Text: {}\\n\\n'.format(tokens))\n",
    "    \n",
    "    \n",
    "# print('Was able to parse {}/{} tweets, {} tweets were not able to be parsed.'.format(valid_count, len(results.tweets.values()), len(results.tweets.values())-valid_count))\n",
    "# print('Average Time to Parse: {:.3f} seconds'.format(sum(times)/len(results.tweets.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81bf7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 73%\n",
    "# 0.018s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "17d08482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(organization_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152c13e",
   "metadata": {},
   "source": [
    "## Relaxed Parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c24377a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import datetime\n",
    "# import torchtext\n",
    "# import dateparser\n",
    "\n",
    "# times = []\n",
    "\n",
    "# valid_count = 0\n",
    "\n",
    "# relaxed_parse = False\n",
    "# attempt_clean_url_entities = True\n",
    "\n",
    "# for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "#     start_time = time.time()\n",
    "#     could_parse = False\n",
    "    \n",
    "#     print(idx)\n",
    "#     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, results.users[tweet.author_id].name) + color.END)\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     if hasattr(tweet, 'entities'):\n",
    "#         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "#     else:\n",
    "#         url_start_idx = -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     if not relaxed_parse:\n",
    "#         relaxed_num_people = None\n",
    "#         relaxed_organization = []\n",
    "#         relaxed_location = None\n",
    "#         relaxed_datetime = None\n",
    "#         for token in tokens:\n",
    "#             # Try parse token as a number?\n",
    "#             if not relaxed_num_people:\n",
    "#                 try:\n",
    "#                     int(token)\n",
    "#                     relaxed_num_people = int(token)\n",
    "#                 except ValueError as e:\n",
    "#                     None\n",
    "                \n",
    "#             if not relaxed_datetime:\n",
    "#                 # Try parse token as a datetime?\n",
    "#                 relaxed_datetime = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "            \n",
    "#             if token in set(organization_list):\n",
    "#                 relaxed_organization.append(token)\n",
    "\n",
    "#     na_str = color.RED + 'N/A' + color.END\n",
    "#     could_parse = (relaxed_num_people is not None) and (relaxed_organization is not None)\n",
    "#     num_people = na_str if not relaxed_num_people else color.GREEN + '{}'.format(relaxed_num_people) + color.END\n",
    "#     organization = na_str if not relaxed_organization else color.GREEN + '/'.join(list(set(relaxed_organization))) + color.END\n",
    "#     location = color.RED + na_str + color.END\n",
    "#     datetime = color.YELLOW + '{}'.format(tweet.created_at) + color.END if not relaxed_datetime else color.GREEN + '{}'.format(relaxed_datetime) + color.END\n",
    "    \n",
    "#     end_time = time.time()\n",
    "    \n",
    "    \n",
    "#     if could_parse:\n",
    "#         print(f'Number of People: {num_people}\\nOrganization: {organization}\\nLocation: {location}\\nDatetime: {datetime}\\n')\n",
    "#         valid_count += 1\n",
    "#     else:\n",
    "#         print(color.RED + 'Unable to parse: {}...\\n'.format(tweet.text[:100]) + color.END)\n",
    "    \n",
    "# print('Was able to parse {}/{} tweets, {} tweets were not able to be parsed.'.format(valid_count, len(results.tweets.values()), len(results.tweets.values())-valid_count))\n",
    "# print('Average Time to Parse: {:.3f} seconds'.format(sum(times)/len(results.tweets.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "60a4331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 82%\n",
    "# 0.302s -> x16.7 slower that strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f114e",
   "metadata": {},
   "source": [
    "# Responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "04d91e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import datetime\n",
    "# import torchtext\n",
    "# import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0af356eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_fields(bot_user, tweet): \n",
    "#     fields = {'num_people':None, 'organization':None, 'location':None, 'datetime':None}\n",
    "#     could_parse = False \n",
    "\n",
    "#     # Check tweet is not self\n",
    "#     if tweet.author_id==bot_user.id:\n",
    "#         return could_parse, _\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     twiff_idxs = [idx for (idx, token) in enumerate(tokens) if token=='twiff']\n",
    "    \n",
    "#     for idx in twiff_idxs:\n",
    "#         if idx+1<len(tokens):\n",
    "#             try:\n",
    "#                 int(tokens[idx+1])\n",
    "#                 tokens = tokens[idx+1:]\n",
    "                \n",
    "#                 # Parse number of people: Can verify\n",
    "#                 fields['num_people'] = int(tokens[0])\n",
    "#                 del tokens[0]\n",
    "                \n",
    "#                 # Parse datetime\n",
    "#                 inferred_datetime = False\n",
    "#                 for idx, token in enumerate(tokens): \n",
    "#                     date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#                     if date is not None:  \n",
    "#                         inferred_datetime = True\n",
    "#                         del tokens[idx]\n",
    "#                         break\n",
    "#                 if date is not None: fields['datetime'] = date.strftime('%Y-%m-%d')\n",
    "                        \n",
    "#                 # Extract organization\n",
    "#                 fields['organization'] = tokens[0]\n",
    "#                 del tokens[0]\n",
    "\n",
    "#                 # Extract location\n",
    "#                 if [idx for (idx, token) in enumerate(tokens) if token=='twiff']:\n",
    "#                     tokens = tokens[:[idx for (idx, token) in enumerate(tokens) if token=='twiff'][0]]\n",
    "#                 fields['location'] = ', '.join(tokens)                 \n",
    "                \n",
    "#                 could_parse = True\n",
    "                \n",
    "#             except ValueError as e:\n",
    "#                 None \n",
    "    \n",
    "#     return could_parse, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b8135840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class User:\n",
    "#     def __init__(self, client:Any, identifier:Union[int,str]):\n",
    "#         try:\n",
    "#             int(identifier)\n",
    "#             self.data = client.get_user(id=int(identifier))['data']\n",
    "#         except ValueError as e:\n",
    "#             self.data = client.get_user(username=identifier)['data']\n",
    "        \n",
    "#         # Variables\n",
    "#         self.id = self.data['id']\n",
    "#         self.name = self.data['name']\n",
    "#         self.username = self.data['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2037693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed87cd",
   "metadata": {},
   "source": [
    "### Parse Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "903da8cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "    \n",
    "#     text = tweet.text\n",
    "    \n",
    "#     text = text[text.find('#twiff')++len('#twiff'):text.find('https')]\n",
    "    \n",
    "#     cleaned_text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(cleaned_text)    \n",
    "    \n",
    "#     try:\n",
    "#         int(tokens[0]) # \n",
    "        \n",
    "#         # Try splitting by ',' first\n",
    "#         tokens = [re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', '', token) for token in text.split(',')]\n",
    "        \n",
    "#         # Parse datetime\n",
    "#         inferred_datetime = False\n",
    "#         datetime_tokens = torchtext.data.get_tokenizer(\"basic_english\")(re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', '', text))    \n",
    "#         for idx, token in enumerate(datetime_tokens): \n",
    "#             date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#             if date is not None:  \n",
    "#                 inferred_datetime = True\n",
    "#                 del tokens[idx]\n",
    "#                 break\n",
    "#         if date is not None: fields['datetime'] = date.strftime('%Y-%m-%d')\n",
    "            \n",
    "#         print(f\"{text} -> {datetime_tokens}:{date}\")\n",
    "        \n",
    "#     except:\n",
    "#         print(f\"Failed\")\n",
    "\n",
    "# # Code\n",
    "# # for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "# #     start_time = time.time()\n",
    "# #     could_parse = False\n",
    "    \n",
    "# #     print(idx)\n",
    "# #     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, results.users[tweet.author_id].name) + color.END)\n",
    "    \n",
    "# #     # Clean up entities\n",
    "# #     if hasattr(tweet, 'entities'):\n",
    "# #         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "# #     else:\n",
    "# #         url_start_idx = -1\n",
    "    \n",
    "# #     # Clean text\n",
    "# #     text = tweet.text[:url_start_idx].strip().lower()\n",
    "# #     text = remove_emoji(text)\n",
    "# #     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "# #     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "# #     # Tokenize the text (could also split but less reliable)\n",
    "# #     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcfed9",
   "metadata": {},
   "source": [
    "### Good Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "31cd6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d8fcaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = list(results.tweets.values())[::-1][0]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text) # reply_settings=\"mentionedUsers\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a39ed4",
   "metadata": {},
   "source": [
    "### Bad Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0f73efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = list(results.tweets.values())[::-1][12]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1720e",
   "metadata": {},
   "source": [
    "### PSA\n",
    "\n",
    "Yesterday found X tweets! Managed to parse X correctly! It took me X to process!\n",
    "\n",
    "We have X users involved in the hashtag! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "326b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let other people reply -> reply_settings=mentionedUsers -> everyone\n",
    "# manually set rate limits to 100/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e332676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy -> \n",
    "# Somewhat ->\n",
    "# Unrelated -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd302e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
