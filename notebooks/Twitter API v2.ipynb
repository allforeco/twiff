{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eb3a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "base_path = os.path.dirname(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\src\\\\twiff\")\n",
    "if not base_path in sys.path:\n",
    "    sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82474fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(format=\"Twitter4Future: [ %(asctime)s ] %(name)s | %(levelname)s | %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S%p\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8753a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'API_KEY': 'vmivCeg7dOdQ45Ku6wN2Dmbqa',\n",
       " 'API_KEY_SECRET': '00wugbciCduP9Ba4wCffmthIc8m7ZmFYb1Wy4Sor5AmJF11Gze',\n",
       " 'BEARER_TOKEN': 'AAAAAAAAAAAAAAAAAAAAANwSXQEAAAAAUCQLIXkyUZDUEbAv%2FaKnOxvNDRI%3DL4t2OpV1FFRkn1uVWUV96rUVrOoRErNRlvGaD4nMwApRHMeGCZ',\n",
       " 'ACCESS_TOKEN': '1466484534430547976-d9FcRgEbz3bgk74IvGX9metx4JSYu1',\n",
       " 'ACCESS_TOKEN_SECRET': 'fQwAOHuvN99K2yaD0NThtXIzmt75QQctujYSeKYt0krfW',\n",
       " 'CLIENT_ID': 'LXJ1V2RuV3RSbkZXWlJWbGZRVk06MTpjaQ',\n",
       " 'CLIENT_SECRET': 'BJXvFrzhBMN_TXLwwxmBFblOWvSSqULP7TyMFOcHijKN6HStWN'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = {}\n",
    "with open(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\docker\\\\search\\\\dev.env\", \"r\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        key, val = line.split(\"=\")\n",
    "        keys[key] = val.strip()\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5e0a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = {\n",
    "#     \"API_KEY\":\"vmivCeg7dOdQ45Ku6wN2Dmbqa\",\n",
    "#     \"API_KEY_SECRET\":\"00wugbciCduP9Ba4wCffmthIc8m7ZmFYb1Wy4Sor5AmJF11Gze\",\n",
    "#     \"BEARER_TOKEN\":\"AAAAAAAAAAAAAAAAAAAAANwSXQEAAAAAUCQLIXkyUZDUEbAv%2FaKnOxvNDRI%3DL4t2OpV1FFRkn1uVWUV96rUVrOoRErNRlvGaD4nMwApRHMeGCZ\",\n",
    "#     \"ACCESS_TOKEN\":\"1466484534430547976-d9FcRgEbz3bgk74IvGX9metx4JSYu1\",\n",
    "#     \"ACCESS_TOKEN_SECRET\":\"fQwAOHuvN99K2yaD0NThtXIzmt75QQctujYSeKYt0krfW\",\n",
    "#     \"CLIENT_ID\":\"LXJ1V2RuV3RSbkZXWlJWbGZRVk06MTpjaQ\",\n",
    "#     \"CLIENT_SECRET\":\"BJXvFrzhBMN_TXLwwxmBFblOWvSSqULP7TyMFOcHijKN6HStWN\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81344de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Twitter4Future: [ 05/29/2022 03:58:26PM ] __main__ | INFO | Session: <requests.sessions.Session object at 0x0000027640425C70>\n",
      "Twitter4Future: [ 05/29/2022 03:58:26PM ] __main__ | INFO | User Agent: Python/3.9.7 Requests/2.26.0 Tweepy/4.4.0\n"
     ]
    }
   ],
   "source": [
    "# Initialise tweepy client\n",
    "from tweepy import Client\n",
    "client = Client(\n",
    "    bearer_token=keys['BEARER_TOKEN'], \n",
    "    consumer_key=keys['API_KEY'], consumer_secret=keys['API_KEY_SECRET'], \n",
    "    access_token=keys['ACCESS_TOKEN'], access_token_secret=keys['ACCESS_TOKEN_SECRET'], \n",
    "    return_type=dict, wait_on_rate_limit=True\n",
    ")\n",
    "log.info(f\"Session: {client.session}\")\n",
    "log.info(f\"User Agent: {client.user_agent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96e8c820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '1466484534430547976',\n",
       "  'name': '@Twiff_bot üåè #Twiff',\n",
       "  'username': 'twiff_bot'}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_user(username=\"twiff_bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a28f4edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Twitter4Future: [ 05/29/2022 02:58:20PM ] twiff.utils.cursor | INFO | Loading... 'Cursor (None): oldest_id=None, newest_id=None'\n",
      "Twitter4Future: [ 05/29/2022 02:58:20PM ] twiff.utils.cursor | INFO | Loading... 'Cursor (\\\\app\\\\output\\\\cursor.json): oldest_id=None, newest_id=None'\n",
      "Twitter4Future: [ 05/29/2022 02:58:21PM ] twiff.utils.cursor | INFO | Updated cursor: 'Cursor (None): oldest_id=1530482360491331587, newest_id=1530621345503002626'\n",
      "Twitter4Future: [ 05/29/2022 02:58:21PM ] twiff.utils.cursor | INFO | Dumping cursor to \"/app/output/cursor.json\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/output/cursor.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20472/418160472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtwiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"search\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"config\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Python Projects\\twiff\\src\\twiff\\search.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(client, cursor, query, max_requests)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mnew_cursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'oldest_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'newest_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mnew_cursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Retrieved {len(tweets)} tweets and {len(users)} associated users. Encountered {len(errors)} errors.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python Projects\\twiff\\src\\twiff\\utils\\cursor.py\u001b[0m in \u001b[0;36m_dump\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dumping cursor to \"{}\"'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moldest_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewest_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/output/cursor.json'"
     ]
    }
   ],
   "source": [
    "# Load configuration for run\n",
    "import json\n",
    "with open(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\scripts\\\\search\\\\search.json\", 'r') as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "from twiff.search import search\n",
    "tweets, users, errors, metadata = search(client=client, **config[\"search\"][\"config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af97d86",
   "metadata": {},
   "source": [
    "# Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e462d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(format=\"[ %(asctime)s ] %(name)s | %(levelname)s | %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S%p\", level=logging.INFO)\n",
    "log.info(f\"Executing: {sys.argv}\")\n",
    "\n",
    "# Ensure src path is in sys path\n",
    "base_path = os.path.dirname(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\src\\\\twiff\")\n",
    "if not base_path in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "\n",
    "# Load configuration for run\n",
    "with open(\"C:\\\\Users\\\\samca\\\\Documents\\\\Python Projects\\\\twiff\\\\tests\\\\search\\\\search.json\", 'r') as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "# Run requested mode\n",
    "if True:\n",
    "    from twiff import search\n",
    "    arg_parser = search.get_arg_parser()\n",
    "    arg_parser.set_defaults(**config)\n",
    "    args = arg_parser.parse_args([])\n",
    "    tweets, users, errors = search.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twiff.parse import parse_tweets\n",
    "\n",
    "for (tweet, parsed) in zip(tweets.values(), parse_tweets(tweets).values()):\n",
    "    print(tweet.text)\n",
    "    print(parsed)\n",
    "    \n",
    "    print('-----')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe48af",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f21d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tweet in list(tweets.values())[::-1]: \n",
    "    print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, 0) + color.END)\n",
    "    print(tweet.text)\n",
    "    print('----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "      u\"\\U0001F600-\\U0001F64F\"\n",
    "      u\"\\U0001F300-\\U0001F5FF\"\n",
    "      u\"\\U0001F680-\\U0001F6FF\"\n",
    "      u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "      u\"\\U00002500-\\U00002BEF\"\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U000024C2-\\U0001F251\"\n",
    "      u\"\\U0001f926-\\U0001f937\"\n",
    "      u\"\\U00010000-\\U0010ffff\"\n",
    "      u\"\\u2640-\\u2642\"\n",
    "      u\"\\u2600-\\u2B55\"\n",
    "      u\"\\u200d\"\n",
    "      u\"\\u23cf\"\n",
    "      u\"\\u23e9\"\n",
    "      u\"\\u231a\"\n",
    "      u\"\\ufe0f\"\n",
    "      u\"\\u3030\"\n",
    "      \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f121d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c75ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from nltk.corpus import words\n",
    "\n",
    "setofwords = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "ls = ['a', 's', 'r', 'n', 'v']\n",
    "speech_constants = {}\n",
    "\n",
    "for constant in ls:\n",
    "    word_set = {word.name().split('.', 1)[0] for word in wn.all_synsets(constant)}\n",
    "    speech_constants = {*speech_constants, *word_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334cf33",
   "metadata": {},
   "source": [
    "## Strict Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import torchtext\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb303a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_people_from_tokens(tokens):\n",
    "    # Search for first token with run of 1\n",
    "    run = 0\n",
    "    num_people = None\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        # Attempt to convert token to integer format - accumulate number of integers encountered.\n",
    "        try:\n",
    "            int(token)\n",
    "            run += 1\n",
    "        except:\n",
    "            if run==1:\n",
    "                num_people = tokens[tdx-1]\n",
    "                break\n",
    "            run = 0\n",
    "        \n",
    "    return num_people\n",
    "\n",
    "def remove_integer_tokens(tokens):   \n",
    "    # Remove any other integer from tokens\n",
    "    int_idxs = []\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        try:\n",
    "            int(token)\n",
    "            int_idxs.append(tdx)\n",
    "        except:\n",
    "            pass\n",
    "    for int_idx in int_idxs[::-1]:\n",
    "        del tokens[int_idx]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def remove_integers_from_tokens(tokens):\n",
    "    for tdx, token in enumerate(tokens):\n",
    "        token = token.strip().lower()\n",
    "        token = remove_emoji(token)\n",
    "        token = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', token)\n",
    "        \n",
    "        sub_tokens = torchtext.data.get_tokenizer(\"basic_english\")(token)\n",
    "        \n",
    "        int_idxs = []\n",
    "        for idx, sub_token in enumerate(sub_tokens):\n",
    "            try:\n",
    "                int(sub_token)\n",
    "                int_idxs.append(idx)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for int_idx in int_idxs[::-1]:\n",
    "            del sub_tokens[int_idx]\n",
    "            \n",
    "        tokens[tdx] = ''.join(sub_tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets(tweets, **kwargs):\n",
    "    parsed_tweets = {}\n",
    "\n",
    "    for idx, (id_str, tweet) in enumerate(tweets.items()):\n",
    "        # Retrieve text from tweet\n",
    "        text = tweet.text.lower()\n",
    "\n",
    "        # Remove data before '#twiff' token\n",
    "        token_idx = text.find('#twiff')\n",
    "        if token_idx:\n",
    "            text = text[token_idx+len('#twiff'):]\n",
    "\n",
    "        # Remove data after 'http' token\n",
    "        token_idx = text.find('http')\n",
    "        if token_idx:\n",
    "            text = text[:token_idx]\n",
    "\n",
    "        # Slight cleaning of tokens\n",
    "        tokens = [item.strip() for item in text.split(',') if item!='']\n",
    "\n",
    "        # Extract number of people\n",
    "        num_people = num_people_from_tokens(tokens)\n",
    "\n",
    "        # Remove integer tokens\n",
    "        tokens = remove_integer_tokens(tokens)\n",
    "\n",
    "        if num_people:\n",
    "            parsed_tweets[id_str] = {\"num_people\":num_people, \"created_at\":tweet.created_at, \"organization\":tokens[0], \"location\":', '.join(tokens[1:])}\n",
    "        else:\n",
    "            parsed_tweets[id_str] = None\n",
    "            \n",
    "    return parsed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tweets = parse_tweets(tweets)\n",
    "\n",
    "for id_str, tweet in parsed_tweets.items():\n",
    "    print(f\"{id_str}: {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(client:Any, parsed_tweets:Any, response_generator:Callable) -> None:\n",
    "    \"\"\" Replies to the author of the parsed tweet as dictated by the provided response_generator.\n",
    "    \n",
    "        Args:\n",
    "            client (tweepy.Client): Registered and X client.\n",
    "            parsed_tweets (Dict): Parsed tweet data in dictionary format.\n",
    "                Key (str): Unique ID corresponding to the tweet.\n",
    "                Value (Dict):\n",
    "                    num_people (int):\n",
    "                    created_at (str):\n",
    "                    organization (str):\n",
    "                    location (str): \n",
    "            response_generator (Callable): Callable function to generate response based on parsed tweet data.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        Example::\n",
    "            >>> tweets = search(...)\n",
    "            >>> parsed_tweets = twiff.parse.parse_tweets(tweets)\n",
    "            >>> reply(client, parsed_tweets, ...) # ...\n",
    "    \"\"\"\n",
    "    for idx, (id_str, parsed_tweet) in enumerate(parsed_tweets.items()):\n",
    "        response = response_generator(parsed_tweet)\n",
    "        client.create_tweet(in_reply_to_tweet_id=tweet.id, text=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# tweet = list(results.tweets.values())[::-1][0]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text) # reply_settings=\"mentionedUsers\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1883c6",
   "metadata": {},
   "source": [
    "### old testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0ccc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# valid_count = 0\n",
    "\n",
    "# strict_parse = False\n",
    "# attempt_clean_url_entities = True\n",
    "\n",
    "# organization_list = []\n",
    "# times = []\n",
    "\n",
    "# for idx, (key, tweet) in enumerate(list(tweets.items())[::-1]):\n",
    "#     start_time = time.time()\n",
    "#     could_parse = False \n",
    "    \n",
    "#     print(idx)\n",
    "#     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, 0) + color.END)\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     if hasattr(tweet, 'entities'):\n",
    "#         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "#     else:\n",
    "#         url_start_idx = -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     # Clean link from tokenized fields\n",
    "#     if 'https' in tokens:\n",
    "#         tokens = tokens[:tokens.index('https')]\n",
    "    \n",
    "#     twiff_idxs = [idx for (idx, token) in enumerate(tokens) if token=='twiff']\n",
    "    \n",
    "#     for idx in twiff_idxs:\n",
    "#         if idx+1<len(tokens):\n",
    "#             try:\n",
    "#                 int(tokens[idx+1])\n",
    "#                 tokens = tokens[idx+1:]\n",
    "#                 valid_count += 1\n",
    "                \n",
    "#                 # Parse number of people: Can verify\n",
    "#                 num_people = int(tokens[0])\n",
    "#                 del tokens[0]\n",
    "                \n",
    "#                 # remove other tokens\n",
    "#                 for token in tokens[::-1]:\n",
    "#                     try:\n",
    "#                         int(token)\n",
    "#                         del tokens[tokens.index(token)]\n",
    "#                     except ValueError as e:\n",
    "#                         None \n",
    "                \n",
    "#                 # Parse datetime\n",
    "#                 inferred_datetime = False\n",
    "#                 for idx, token in enumerate(tokens): \n",
    "#                     date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#                     if date is not None:  \n",
    "#                         inferred_datetime = True\n",
    "#                         del tokens[idx]\n",
    "#                         break\n",
    "#                 if date is not None:\n",
    "#                     date = color.GREEN + '{}'.format(date) + color.END\n",
    "#                 else:\n",
    "#                     date = color.YELLOW + '{}'.format(datetime.datetime.strptime(tweet.created_at, '%Y-%m-%dT%H:%M:%S.%fZ')) + color.END\n",
    "                    \n",
    "#                 # Extract organization\n",
    "#                 organization = tokens[0]\n",
    "#                 organization_list.append(organization)\n",
    "#                 del tokens[0]\n",
    "\n",
    "#                 # Extract location\n",
    "#                 if [idx for (idx, token) in enumerate(tokens) if token=='twiff']:\n",
    "#                     tokens = tokens[:[idx for (idx, token) in enumerate(tokens) if token=='twiff'][0]]\n",
    "#                 location = ', '.join(tokens)                 \n",
    "                \n",
    "#                 could_parse = True\n",
    "                \n",
    "#             except ValueError as e:\n",
    "#                 None \n",
    "                \n",
    "#     end_time = time.time()\n",
    "#     times.append(end_time-start_time)\n",
    "#     print('Parsing tweet took: {:.3f} seconds'.format(end_time-start_time))      \n",
    "    \n",
    "#     if could_parse:\n",
    "#         print(color.GREEN + 'Successfully parsed tweet.' + color.END)\n",
    "#         print(f'Number of People: {num_people}\\nOrganization: {organization}\\nLocation: {location}\\nDatetime: {date}\\n')\n",
    "#     else:\n",
    "#         print(color.RED + 'Unable to parse: {}...'.format(tweet.text[:100]) + color.END)\n",
    "#         print('Tokenized Text: {}\\n\\n'.format(tokens))\n",
    "    \n",
    "    \n",
    "# print('Was able to parse {}/{} tweets, {} tweets were not able to be parsed.'.format(valid_count, len(results.tweets.values()), len(results.tweets.values())-valid_count))\n",
    "# print('Average Time to Parse: {:.3f} seconds'.format(sum(times)/len(results.tweets.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 73%\n",
    "# 0.018s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d08482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(organization_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152c13e",
   "metadata": {},
   "source": [
    "## Relaxed Parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24377a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import datetime\n",
    "# import torchtext\n",
    "# import dateparser\n",
    "\n",
    "# times = []\n",
    "\n",
    "# valid_count = 0\n",
    "\n",
    "# relaxed_parse = False\n",
    "# attempt_clean_url_entities = True\n",
    "\n",
    "# for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "#     start_time = time.time()\n",
    "#     could_parse = False\n",
    "    \n",
    "#     print(idx)\n",
    "#     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, results.users[tweet.author_id].name) + color.END)\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     if hasattr(tweet, 'entities'):\n",
    "#         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "#     else:\n",
    "#         url_start_idx = -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     if not relaxed_parse:\n",
    "#         relaxed_num_people = None\n",
    "#         relaxed_organization = []\n",
    "#         relaxed_location = None\n",
    "#         relaxed_datetime = None\n",
    "#         for token in tokens:\n",
    "#             # Try parse token as a number?\n",
    "#             if not relaxed_num_people:\n",
    "#                 try:\n",
    "#                     int(token)\n",
    "#                     relaxed_num_people = int(token)\n",
    "#                 except ValueError as e:\n",
    "#                     None\n",
    "                \n",
    "#             if not relaxed_datetime:\n",
    "#                 # Try parse token as a datetime?\n",
    "#                 relaxed_datetime = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "            \n",
    "#             if token in set(organization_list):\n",
    "#                 relaxed_organization.append(token)\n",
    "\n",
    "#     na_str = color.RED + 'N/A' + color.END\n",
    "#     could_parse = (relaxed_num_people is not None) and (relaxed_organization is not None)\n",
    "#     num_people = na_str if not relaxed_num_people else color.GREEN + '{}'.format(relaxed_num_people) + color.END\n",
    "#     organization = na_str if not relaxed_organization else color.GREEN + '/'.join(list(set(relaxed_organization))) + color.END\n",
    "#     location = color.RED + na_str + color.END\n",
    "#     datetime = color.YELLOW + '{}'.format(tweet.created_at) + color.END if not relaxed_datetime else color.GREEN + '{}'.format(relaxed_datetime) + color.END\n",
    "    \n",
    "#     end_time = time.time()\n",
    "    \n",
    "    \n",
    "#     if could_parse:\n",
    "#         print(f'Number of People: {num_people}\\nOrganization: {organization}\\nLocation: {location}\\nDatetime: {datetime}\\n')\n",
    "#         valid_count += 1\n",
    "#     else:\n",
    "#         print(color.RED + 'Unable to parse: {}...\\n'.format(tweet.text[:100]) + color.END)\n",
    "    \n",
    "# print('Was able to parse {}/{} tweets, {} tweets were not able to be parsed.'.format(valid_count, len(results.tweets.values()), len(results.tweets.values())-valid_count))\n",
    "# print('Average Time to Parse: {:.3f} seconds'.format(sum(times)/len(results.tweets.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 82%\n",
    "# 0.302s -> x16.7 slower that strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f114e",
   "metadata": {},
   "source": [
    "# Responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d91e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import datetime\n",
    "# import torchtext\n",
    "# import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af356eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_fields(bot_user, tweet): \n",
    "#     fields = {'num_people':None, 'organization':None, 'location':None, 'datetime':None}\n",
    "#     could_parse = False \n",
    "\n",
    "#     # Check tweet is not self\n",
    "#     if tweet.author_id==bot_user.id:\n",
    "#         return could_parse, _\n",
    "    \n",
    "#     # Clean up entities\n",
    "#     url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "    \n",
    "#     # Clean text\n",
    "#     text = tweet.text[:url_start_idx].strip().lower()\n",
    "#     text = remove_emoji(text)\n",
    "#     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "#     # Tokenize the text (could also split but less reliable)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    \n",
    "    \n",
    "#     twiff_idxs = [idx for (idx, token) in enumerate(tokens) if token=='twiff']\n",
    "    \n",
    "#     for idx in twiff_idxs:\n",
    "#         if idx+1<len(tokens):\n",
    "#             try:\n",
    "#                 int(tokens[idx+1])\n",
    "#                 tokens = tokens[idx+1:]\n",
    "                \n",
    "#                 # Parse number of people: Can verify\n",
    "#                 fields['num_people'] = int(tokens[0])\n",
    "#                 del tokens[0]\n",
    "                \n",
    "#                 # Parse datetime\n",
    "#                 inferred_datetime = False\n",
    "#                 for idx, token in enumerate(tokens): \n",
    "#                     date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#                     if date is not None:  \n",
    "#                         inferred_datetime = True\n",
    "#                         del tokens[idx]\n",
    "#                         break\n",
    "#                 if date is not None: fields['datetime'] = date.strftime('%Y-%m-%d')\n",
    "                        \n",
    "#                 # Extract organization\n",
    "#                 fields['organization'] = tokens[0]\n",
    "#                 del tokens[0]\n",
    "\n",
    "#                 # Extract location\n",
    "#                 if [idx for (idx, token) in enumerate(tokens) if token=='twiff']:\n",
    "#                     tokens = tokens[:[idx for (idx, token) in enumerate(tokens) if token=='twiff'][0]]\n",
    "#                 fields['location'] = ', '.join(tokens)                 \n",
    "                \n",
    "#                 could_parse = True\n",
    "                \n",
    "#             except ValueError as e:\n",
    "#                 None \n",
    "    \n",
    "#     return could_parse, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8135840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class User:\n",
    "#     def __init__(self, client:Any, identifier:Union[int,str]):\n",
    "#         try:\n",
    "#             int(identifier)\n",
    "#             self.data = client.get_user(id=int(identifier))['data']\n",
    "#         except ValueError as e:\n",
    "#             self.data = client.get_user(username=identifier)['data']\n",
    "        \n",
    "#         # Variables\n",
    "#         self.id = self.data['id']\n",
    "#         self.name = self.data['name']\n",
    "#         self.username = self.data['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed87cd",
   "metadata": {},
   "source": [
    "### Parse Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903da8cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "    \n",
    "#     text = tweet.text\n",
    "    \n",
    "#     text = text[text.find('#twiff')++len('#twiff'):text.find('https')]\n",
    "    \n",
    "#     cleaned_text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "#     tokens = torchtext.data.get_tokenizer(\"basic_english\")(cleaned_text)    \n",
    "    \n",
    "#     try:\n",
    "#         int(tokens[0]) # \n",
    "        \n",
    "#         # Try splitting by ',' first\n",
    "#         tokens = [re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', '', token) for token in text.split(',')]\n",
    "        \n",
    "#         # Parse datetime\n",
    "#         inferred_datetime = False\n",
    "#         datetime_tokens = torchtext.data.get_tokenizer(\"basic_english\")(re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', '', text))    \n",
    "#         for idx, token in enumerate(datetime_tokens): \n",
    "#             date = dateparser.parse(token, settings={'STRICT_PARSING':True})\n",
    "#             if date is not None:  \n",
    "#                 inferred_datetime = True\n",
    "#                 del tokens[idx]\n",
    "#                 break\n",
    "#         if date is not None: fields['datetime'] = date.strftime('%Y-%m-%d')\n",
    "            \n",
    "#         print(f\"{text} -> {datetime_tokens}:{date}\")\n",
    "        \n",
    "#     except:\n",
    "#         print(f\"Failed\")\n",
    "\n",
    "# # Code\n",
    "# # for idx, (key, tweet) in enumerate(list(results.tweets.items())[::-1]):\n",
    "# #     start_time = time.time()\n",
    "# #     could_parse = False\n",
    "    \n",
    "# #     print(idx)\n",
    "# #     print(color.BOLD + '{} @ {} by {}' .format(tweet.id, tweet.created_at, results.users[tweet.author_id].name) + color.END)\n",
    "    \n",
    "# #     # Clean up entities\n",
    "# #     if hasattr(tweet, 'entities'):\n",
    "# #         url_start_idx = tweet.entities['urls'][0]['start'] if 'urls' in tweet.entities and attempt_clean_url_entities else -1\n",
    "# #     else:\n",
    "# #         url_start_idx = -1\n",
    "    \n",
    "# #     # Clean text\n",
    "# #     text = tweet.text[:url_start_idx].strip().lower()\n",
    "# #     text = remove_emoji(text)\n",
    "# #     text = re.sub('[,:;(){}##/?@!.\\[\\]&%$*^\\+=|<>`~\"\\']', ' ', text)\n",
    "# #     text = re.sub('  ', ' ', text)\n",
    "    \n",
    "# #     # Tokenize the text (could also split but less reliable)\n",
    "# #     tokens = torchtext.data.get_tokenizer(\"basic_english\")(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcfed9",
   "metadata": {},
   "source": [
    "### Good Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def response(client, bot_user, tweet): \n",
    "#     # User\n",
    "#     user = User(client, int(tweet.author_id))\n",
    "#     print('Tweet from: {}'.format(user.name))\n",
    "    \n",
    "#     # Extract Fields\n",
    "#     could_parse, fields = extract_fields(bot_user, tweet)\n",
    "#     print('could_parse={}, fields={}'.format(could_parse, fields))\n",
    "    \n",
    "#     header = f\"Thanks @{user.username} for your climate action üåè and using #twiff!\"\n",
    "#     who = f\"I'm @{bot_user.username}, an automated bot experimenting how to read your tweets for #FFFMapCount!\"\n",
    "\n",
    "#     success = f\"I've managed to extract some fields from your tweet! \\U0001F601\"\n",
    "#     failed = f\"I wasn't able to extract any fields from your tweet! \\U0001F622\"\n",
    "\n",
    "#     fields = f\"People: {fields['num_people']}\\nOrg: {fields['organization']}\\nLocation: {fields['location']}\\nDate: {fields['datetime']}\"\n",
    "\n",
    "#     success_format = f\"If any fields are incorrect please reply using the following format: \"\n",
    "#     failed_format = f\"To have your action registered please reply using the following format: \"\n",
    "    \n",
    "#     twiff_format = f\"#twiff [Number of People], [Organization], [Location], [Datetime]\"\n",
    "#     example = f\"Here is an example of a tweet I picked up recently:\\n{0}\"\n",
    "    \n",
    "#     if could_parse:\n",
    "#         text = f\"{header} \\U0001F601\\n\\n{fields}\\n\\n{success_format+twiff_format}\"\n",
    "#     else: \n",
    "#         text = f\"{header}\\n\\n{failed}\\n\\n{failed_format+twiff_format}\"\n",
    "    \n",
    "#     print('Length: {}'.format(len(text)))\n",
    "#     print('\\n-----\\n{}'.format(text))\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = list(results.tweets.values())[::-1][0]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text) # reply_settings=\"mentionedUsers\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a39ed4",
   "metadata": {},
   "source": [
    "### Bad Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f73efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = list(results.tweets.values())[::-1][12]\n",
    "# bot_user = User(client, 'twiff_bot')\n",
    "\n",
    "# text = response(client, bot_user, tweet)\n",
    "\n",
    "# #client.like(tweet.id)\n",
    "# #client.create_tweet(in_reply_to_tweet_id=tweet.id, text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1720e",
   "metadata": {},
   "source": [
    "### PSA\n",
    "\n",
    "Yesterday found X tweets! Managed to parse X correctly! It took me X to process!\n",
    "\n",
    "We have X users involved in the hashtag! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let other people reply -> reply_settings=mentionedUsers -> everyone\n",
    "# manually set rate limits to 100/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy -> \n",
    "# Somewhat ->\n",
    "# Unrelated -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd302e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
